{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37330,
     "status": "ok",
     "timestamp": 1747476986743,
     "user": {
      "displayName": "Tejaswi Shetty",
      "userId": "11404043263836268655"
     },
     "user_tz": -330
    },
    "id": "QfFcmoVsu9xY",
    "outputId": "07642e8e-76ca-4116-dedd-5d57299e2f3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.9/62.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.9/18.9 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.9/194.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain_community tiktoken langchain-openai langchainhub chromadb langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1096,
     "status": "ok",
     "timestamp": 1747478150979,
     "user": {
      "displayName": "Tejaswi Shetty",
      "userId": "11404043263836268655"
     },
     "user_tz": -330
    },
    "id": "NxOVm9xT0DwE",
    "outputId": "c26cafd0-c1a8-4087-f1da-bab2170796d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=\"<API_KEY>\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=\"RAG_ADVANCED\"\n",
    "os.environ['OPENAI_API_KEY'] = \"<API_KEY>\"\n",
    "\n",
    "\n",
    "from langsmith import utils\n",
    "utils.tracing_is_enabled()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6369,
     "status": "ok",
     "timestamp": 1747478167411,
     "user": {
      "displayName": "Tejaswi Shetty",
      "userId": "11404043263836268655"
     },
     "user_tz": -330
    },
    "id": "97bOhYZJ0J8P",
    "outputId": "c423a5aa-4613-4626-de1c-1fcf0f9201e2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 493,
     "status": "ok",
     "timestamp": 1747478192889,
     "user": {
      "displayName": "Tejaswi Shetty",
      "userId": "11404043263836268655"
     },
     "user_tz": -330
    },
    "id": "AslIM3BP0RtJ"
   },
   "outputs": [],
   "source": [
    "#Load Documents\n",
    "loader = WebBaseLoader(\n",
    "    web_paths = (\"https://lilianweng.github.io/posts/2023-06-23-agent/\", ),\n",
    "    bs_kwargs = dict(\n",
    "        parse_only = bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 4083,
     "status": "ok",
     "timestamp": 1747478216468,
     "user": {
      "displayName": "Tejaswi Shetty",
      "userId": "11404043263836268655"
     },
     "user_tz": -330
    },
    "id": "HcynWnTa0Srm"
   },
   "outputs": [],
   "source": [
    "#Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 300, chunk_overlap=50)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "#Embed\n",
    "embedding = OpenAIEmbeddings(model = \"text-embedding-3-small\")\n",
    "\n",
    "vectorestore = Chroma.from_documents(documents=splits, embedding = embedding, persist_directory=\"./db004\")\n",
    "\n",
    "#Retriever\n",
    "retriever = vectorestore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1747478589923,
     "user": {
      "displayName": "Tejaswi Shetty",
      "userId": "11404043263836268655"
     },
     "user_tz": -330
    },
    "id": "NR7KZTwc1FjQ"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "#Decomposition\n",
    "template = \"\"\"You are a helful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to:{question}\\n\n",
    "Output (3 queries):\"\"\"\n",
    "\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2641,
     "status": "ok",
     "timestamp": 1747478838028,
     "user": {
      "displayName": "Tejaswi Shetty",
      "userId": "11404043263836268655"
     },
     "user_tz": -330
    },
    "id": "BbtZqlBV1276"
   },
   "outputs": [],
   "source": [
    "#LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "#Chain\n",
    "generate_queries_decomposition = (\n",
    "    prompt_decomposition\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "#Run\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "questions = generate_queries_decomposition.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1747478847725,
     "user": {
      "displayName": "Tejaswi Shetty",
      "userId": "11404043263836268655"
     },
     "user_tz": -330
    },
    "id": "TBzx_K_n2m9g",
    "outputId": "38b058d6-50a3-4857-d14b-4e62015f54f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What are the key components of a large language model (LLM) used in autonomous agent systems?',\n",
       " '2. How do perception and sensor integration function within an LLM-powered autonomous agent system?',\n",
       " '3. What role does decision-making and planning play in the architecture of an LLM-based autonomous agent?']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1747479100854,
     "user": {
      "displayName": "Tejaswi Shetty",
      "userId": "11404043263836268655"
     },
     "user_tz": -330
    },
    "id": "1Td3ZLv92zdQ"
   },
   "outputs": [],
   "source": [
    "#Prompt\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question:\n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question : \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 31605,
     "status": "ok",
     "timestamp": 1747479886612,
     "user": {
      "displayName": "Tejaswi Shetty",
      "userId": "11404043263836268655"
     },
     "user_tz": -330
    },
    "id": "th8hniEC3zN9"
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "  \"\"\"Format Q and A pair\"\"\"\n",
    "\n",
    "  formatted_string = \"\"\n",
    "  formatted_string += f\"Question: {question} \\nAnswer:{answer}\\n\\n\"\n",
    "  return formatted_string.strip()\n",
    "\n",
    "#LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "\n",
    "for q in questions:\n",
    "  rag_chain = (\n",
    "      {\"context\":itemgetter(\"question\") | retriever,\n",
    "       \"question\":itemgetter(\"question\"),\n",
    "       \"q_a_pairs\":itemgetter(\"q_a_pairs\")}\n",
    "      | decomposition_prompt\n",
    "      | llm\n",
    "      | StrOutputParser()\n",
    "  )\n",
    "  answer = rag_chain.invoke({\"question\":q, \"q_a_pairs\":q_a_pairs})\n",
    "  q_a_pair = format_qa_pair(q, answer)\n",
    "  q_a_pairs = q_a_pair + \"\\n---\\n\" + q_a_pair\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "executionInfo": {
     "elapsed": 66,
     "status": "ok",
     "timestamp": 1747479890459,
     "user": {
      "displayName": "Tejaswi Shetty",
      "userId": "11404043263836268655"
     },
     "user_tz": -330
    },
    "id": "qRYojtJk6oXe",
    "outputId": "ba99a0ff-6ada-4c66-c6bf-e5a08c62a389"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"In the architecture of an LLM-based autonomous agent, decision-making and planning are fundamental components that enable the agent to operate effectively and autonomously in dynamic environments. Here’s how these elements function within the system:\\n\\n1. **Decision-Making**: The LLM serves as the core decision-making engine of the autonomous agent. It processes sensory data and contextual information to generate responses and determine the best course of action. The LLM utilizes its understanding of language and context to evaluate different options, weigh potential outcomes, and make informed decisions that align with the agent's goals. This capability is crucial for adapting to real-time changes in the environment and responding appropriately to various situations.\\n\\n2. **Planning**: Planning involves the formulation of a sequence of actions that the agent needs to take to achieve specific objectives. In LLM-based systems, planning can be enhanced through various mechanisms:\\n   - **Internal Planning**: The LLM can generate plans based on its understanding of the environment and the tasks at hand. This involves predicting the consequences of different actions and selecting the most effective path to reach a goal.\\n   - **External Planning**: Some architectures may incorporate external classical planners that utilize formal planning languages, such as the Planning Domain Definition Language (PDDL). This approach allows the agent to handle more complex, long-horizon planning tasks by breaking them down into manageable steps and leveraging established planning algorithms.\\n\\n3. **Integration with Memory**: Decision-making and planning are further supported by the agent's memory system, which stores past experiences and contextual information. This memory allows the agent to learn from previous interactions, improving its decision-making and planning capabilities over time. By recalling relevant experiences, the agent can make more informed choices and adapt its strategies based on what has worked in the past.\\n\\n4. **Reflection Mechanisms**: Reflection mechanisms enable the agent to evaluate its past decisions and plans, fostering continuous improvement. By analyzing the outcomes of its actions, the agent can refine its decision-making processes and planning strategies, leading to better performance in future tasks.\\n\\n5. **Reliability and Validation**: Given the potential for errors in decision-making and planning, reliability mechanisms are essential. These may include validation processes to ensure that the decisions made and plans generated are appropriate and feasible based on the current context and sensory inputs.\\n\\nIn summary, decision-making and planning are integral to the architecture of an LLM-based autonomous agent, allowing it to interpret its environment, formulate strategies, and adapt its behavior in pursuit of its goals. These processes are enhanced by memory, reflection, and reliability mechanisms, contributing to the agent's overall effectiveness and autonomy.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPH6IBYAo5CFdtUZzktSIZh",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
