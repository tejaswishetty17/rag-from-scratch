{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40262,
     "status": "ok",
     "timestamp": 1747465014375,
     "user": {
      "displayName": "Tejaswi Shetty",
      "userId": "11404043263836268655"
     },
     "user_tz": -330
    },
    "id": "hve-7zinBsmN",
    "outputId": "77d75ac2-c2da-4d85-8cc9-d30212ef3cb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.9/62.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.9/18.9 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.9/194.9 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain_community tiktoken langchain-openai langchainhub chromadb langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 702,
     "status": "ok",
     "timestamp": 1747465181314,
     "user": {
      "displayName": "Tejaswi Shetty",
      "userId": "11404043263836268655"
     },
     "user_tz": -330
    },
    "id": "7boze910B1U_",
    "outputId": "50e71aba-a599-4ede-eaab-82f68f570f16"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=\"<API_KEY\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=\"RAG_ADVANCED\"\n",
    "os.environ['OPENAI_API_KEY'] = \"<API_KEY>\"\n",
    "\n",
    "\n",
    "from langsmith import utils\n",
    "utils.tracing_is_enabled()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2903,
     "status": "ok",
     "timestamp": 1747465187302,
     "user": {
      "displayName": "Tejaswi Shetty",
      "userId": "11404043263836268655"
     },
     "user_tz": -330
    },
    "id": "OZz7DdSNB5sH",
    "outputId": "ec632976-3aa8-4080-9756-f69ea10b53f6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 330,
     "status": "ok",
     "timestamp": 1747465216539,
     "user": {
      "displayName": "Tejaswi Shetty",
      "userId": "11404043263836268655"
     },
     "user_tz": -330
    },
    "id": "mChbHsvDCuK2"
   },
   "outputs": [],
   "source": [
    "#Load Documents\n",
    "loader = WebBaseLoader(\n",
    "    web_paths = (\"https://lilianweng.github.io/posts/2023-06-23-agent/\", ),\n",
    "    bs_kwargs = dict(\n",
    "        parse_only = bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 6354,
     "status": "ok",
     "timestamp": 1747465252846,
     "user": {
      "displayName": "Tejaswi Shetty",
      "userId": "11404043263836268655"
     },
     "user_tz": -330
    },
    "id": "qxY4oG7RC0Qd"
   },
   "outputs": [],
   "source": [
    "#Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 300, chunk_overlap=50)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "#Embed\n",
    "embedding = OpenAIEmbeddings(model = \"text-embedding-3-small\")\n",
    "\n",
    "vectorestore = Chroma.from_documents(documents=splits, embedding = embedding, persist_directory=\"./db001\")\n",
    "\n",
    "#Retriever\n",
    "retriever = vectorestore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1747465415567,
     "user": {
      "displayName": "Tejaswi Shetty",
      "userId": "11404043263836268655"
     },
     "user_tz": -330
    },
    "id": "FQq9KL6mC-rY"
   },
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "  context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "  return context\n",
    "\n",
    "#Define LLM and Prompt for Answering\n",
    "llm = ChatOpenAI(model = \"gpt-4o-mini\", temperature=0)\n",
    "#Prompt\n",
    "answer_prompt = ChatPromptTemplate.from_template(\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say you do not know.\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Question:{question}\n",
    "Context:{context}\n",
    "Answer:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "asnwer_chain = answer_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3388,
     "status": "ok",
     "timestamp": 1747466256085,
     "user": {
      "displayName": "Tejaswi Shetty",
      "userId": "11404043263836268655"
     },
     "user_tz": -330
    },
    "id": "HvJtwo_xDgXZ",
    "outputId": "0c548d92-70d0-4c3a-d2e7-8e516dcb942b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewritten Query: What is task decomposition for LLM agents in Langchain?\n",
      "Original query: I am a data scientist, I take a lot of trainings and right now learning langchain. Can you tell me what is task decomposition for LLM agents in Langchain?\n",
      "Answer:  Task decomposition for LLM agents in Langchain involves breaking down complex tasks into smaller, manageable subgoals. This process is facilitated by the LLM, which parses user requests into multiple tasks with attributes like task type, ID, dependencies, and arguments. Decomposition can be achieved through simple prompting, task-specific instructions, or human inputs.\n"
     ]
    }
   ],
   "source": [
    "rewrite_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful assistant that improves user queries for information retrieval.\n",
    "\n",
    "    Given a user's input, reqrite it into a clear, standalone question that can be used to rewrite relevant documents.\n",
    "    Remove any conversational fluff or non-essential context.\n",
    "\n",
    "    User input:\n",
    "    {question}\n",
    "\n",
    "    Rewrite question:\"\"\"\n",
    ")\n",
    "rewrite_chain = rewrite_prompt | ChatOpenAI(model = \"gpt-4o-mini\", temperature=0) | StrOutputParser()\n",
    "\n",
    "#RAG Flow with rewrite\n",
    "query = \"I am a data scientist, I take a lot of trainings and right now learning langchain. Can you tell me what is task decomposition for LLM agents in Langchain?\"\n",
    "\n",
    "#Rewrite the query\n",
    "rewritten_query = rewrite_chain.invoke({\"question\":query})\n",
    "print(\"Rewritten Query:\", rewritten_query)\n",
    "\n",
    "#Get relavant documents for the rewritten query\n",
    "retrieved_docs = retriever.invoke(rewritten_query)\n",
    "\n",
    "#Format context and sources\n",
    "context = format_docs(retrieved_docs)\n",
    "\n",
    "#Get final answer from LLM\n",
    "answer = asnwer_chain.invoke({\"context\":context, \"question\":rewritten_query})\n",
    "\n",
    "#Print Results\n",
    "print(\"Original query:\", query)\n",
    "print(\"Answer: \", answer)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMhwCoMCKEQJBD8utux4Fm5",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
